{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8MeqVASIxKBH"
      },
      "outputs": [],
      "source": [
        "! pip install pocketflow>=0.0.1\n",
        "! pip install aiohttp>=3.8.0\n",
        "! pip install openai>=1.0.0\n",
        "! pip install duckduckgo-search>=7.5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUp_sNU1xKBI",
        "outputId": "a647f919-b253-48c8-c132-5eef582e29c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Testing call_llm\n",
            "## Prompt: In a few words, what is the meaning of life?\n"
          ]
        },
        {
          "ename": "PermissionDeniedError",
          "evalue": "Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn a few words, what is the meaning of life?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m call_llm(prompt)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Testing search_web\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mcall_llm\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_llm\u001b[39m(prompt):\n\u001b[0;32m      7\u001b[0m     client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour-api-key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     r \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      9\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[0;32m     11\u001b[0m     )\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
            "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1156\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[0;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m   1155\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   1157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1158\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m   1159\u001b[0m             {\n\u001b[0;32m   1160\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   1161\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   1162\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m   1163\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   1164\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   1165\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   1166\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   1167\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   1168\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   1169\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   1170\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   1171\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m   1172\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   1173\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   1174\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m   1175\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   1176\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[0;32m   1177\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m   1178\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   1179\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[0;32m   1180\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   1181\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   1182\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   1183\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   1184\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   1185\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   1186\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   1187\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   1188\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   1189\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   1190\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   1191\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   1192\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[0;32m   1193\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m   1194\u001b[0m             },\n\u001b[0;32m   1195\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[0;32m   1196\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m   1197\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[0;32m   1198\u001b[0m         ),\n\u001b[0;32m   1199\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   1200\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1201\u001b[0m         ),\n\u001b[0;32m   1202\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   1203\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1204\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m   1205\u001b[0m     )\n",
            "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
            "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;31mPermissionDeniedError\u001b[0m: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}"
          ]
        }
      ],
      "source": [
        "# utils.py\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def call_llm(prompt):\n",
        "    client = OpenAI(api_key=\"your-api-key\")\n",
        "    r = client.chat.completions.create(\n",
        "        model=\"deepseek\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return r.choices[0].message.content\n",
        "\n",
        "def search_web(query):\n",
        "    results = DDGS().text(query, max_results=5)\n",
        "    # Convert results to a string\n",
        "    results_str = \"\\n\\n\".join([f\"Title: {r['title']}\\nURL: {r['href']}\\nSnippet: {r['body']}\" for r in results])\n",
        "    return results_str\n",
        "\n",
        "print(\"## Testing call_llm\")\n",
        "prompt = \"In a few words, what is the meaning of life?\"\n",
        "print(f\"## Prompt: {prompt}\")\n",
        "response = call_llm(prompt)\n",
        "print(f\"## Response: {response}\")\n",
        "\n",
        "print(\"## Testing search_web\")\n",
        "query = \"Who won the Nobel Prize in Physics 2024?\"\n",
        "print(f\"## Query: {query}\")\n",
        "results = search_web(query)\n",
        "print(f\"## Results: {results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0ETd4C2xKBI"
      },
      "outputs": [],
      "source": [
        "# nodes.py\n",
        "from pocketflow import Node\n",
        "import yaml\n",
        "\n",
        "class DecideAction(Node):\n",
        "    def prep(self, shared):\n",
        "        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n",
        "        # Get the current context (default to \"No previous search\" if none exists)\n",
        "        context = shared.get(\"context\", \"No previous search\")\n",
        "        # Get the question from the shared store\n",
        "        question = shared[\"question\"]\n",
        "        # Return both for the exec step\n",
        "        return question, context\n",
        "\n",
        "    def exec(self, inputs):\n",
        "        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n",
        "        question, context = inputs\n",
        "\n",
        "        print(f\"🤔 Agent deciding what to do next...\")\n",
        "\n",
        "        # Create a prompt to help the LLM decide what to do next with proper yaml formatting\n",
        "        prompt = f\"\"\"\n",
        "### CONTEXT\n",
        "You are a research assistant that can search the web.\n",
        "Question: {question}\n",
        "Previous Research: {context}\n",
        "\n",
        "### ACTION SPACE\n",
        "[1] search\n",
        "  Description: Look up more information on the web\n",
        "  Parameters:\n",
        "    - query (str): What to search for\n",
        "\n",
        "[2] answer\n",
        "  Description: Answer the question with current knowledge\n",
        "  Parameters:\n",
        "    - answer (str): Final answer to the question\n",
        "\n",
        "## NEXT ACTION\n",
        "Decide the next action based on the context and available actions.\n",
        "Return your response in this format:\n",
        "\n",
        "```yaml\n",
        "thinking: |\n",
        "    <your step-by-step reasoning process>\n",
        "action: search OR answer\n",
        "reason: <why you chose this action>\n",
        "answer: <if action is answer>\n",
        "search_query: <specific search query if action is search>\n",
        "```\n",
        "IMPORTANT: Make sure to:\n",
        "1. Use proper indentation (4 spaces) for all multi-line fields\n",
        "2. Use the | character for multi-line text fields\n",
        "3. Keep single-line fields without the | character\n",
        "\"\"\"\n",
        "\n",
        "        # Call the LLM to make a decision\n",
        "        response = call_llm(prompt)\n",
        "\n",
        "        # Parse the response to get the decision\n",
        "        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n",
        "        decision = yaml.safe_load(yaml_str)\n",
        "\n",
        "        return decision\n",
        "\n",
        "    def post(self, shared, prep_res, exec_res):\n",
        "        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n",
        "        # If LLM decided to search, save the search query\n",
        "        if exec_res[\"action\"] == \"search\":\n",
        "            shared[\"search_query\"] = exec_res[\"search_query\"]\n",
        "            print(f\"🔍 Agent decided to search for: {exec_res['search_query']}\")\n",
        "        else:\n",
        "            shared[\"context\"] = exec_res[\"answer\"] #save the context if LLM gives the answer without searching.\n",
        "            print(f\"💡 Agent decided to answer the question\")\n",
        "\n",
        "        # Return the action to determine the next node in the flow\n",
        "        return exec_res[\"action\"]\n",
        "\n",
        "class SearchWeb(Node):\n",
        "    def prep(self, shared):\n",
        "        \"\"\"Get the search query from the shared store.\"\"\"\n",
        "        return shared[\"search_query\"]\n",
        "\n",
        "    def exec(self, search_query):\n",
        "        \"\"\"Search the web for the given query.\"\"\"\n",
        "        # Call the search utility function\n",
        "        print(f\"🌐 Searching the web for: {search_query}\")\n",
        "        results = search_web(search_query)\n",
        "        return results\n",
        "\n",
        "    def post(self, shared, prep_res, exec_res):\n",
        "        \"\"\"Save the search results and go back to the decision node.\"\"\"\n",
        "        # Add the search results to the context in the shared store\n",
        "        previous = shared.get(\"context\", \"\")\n",
        "        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n",
        "\n",
        "        print(f\"📚 Found information, analyzing results...\")\n",
        "\n",
        "        # Always go back to the decision node after searching\n",
        "        return \"decide\"\n",
        "\n",
        "class AnswerQuestion(Node):\n",
        "    def prep(self, shared):\n",
        "        \"\"\"Get the question and context for answering.\"\"\"\n",
        "        return shared[\"question\"], shared.get(\"context\", \"\")\n",
        "\n",
        "    def exec(self, inputs):\n",
        "        \"\"\"Call the LLM to generate a final answer.\"\"\"\n",
        "        question, context = inputs\n",
        "\n",
        "        print(f\"✍️ Crafting final answer...\")\n",
        "\n",
        "        # Create a prompt for the LLM to answer the question\n",
        "        prompt = f\"\"\"\n",
        "### CONTEXT\n",
        "Based on the following information, answer the question.\n",
        "Question: {question}\n",
        "Research: {context}\n",
        "\n",
        "## YOUR ANSWER:\n",
        "Provide a comprehensive answer using the research results.\n",
        "\"\"\"\n",
        "        # Call the LLM to generate an answer\n",
        "        answer = call_llm(prompt)\n",
        "        return answer\n",
        "\n",
        "    def post(self, shared, prep_res, exec_res):\n",
        "        \"\"\"Save the final answer and complete the flow.\"\"\"\n",
        "        # Save the answer in the shared store\n",
        "        shared[\"answer\"] = exec_res\n",
        "\n",
        "        print(f\"✅ Answer generated successfully\")\n",
        "\n",
        "        # We're done - no need to continue the flow\n",
        "        return \"done\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B4jCAmXxKBI"
      },
      "outputs": [],
      "source": [
        "# flow.py\n",
        "from pocketflow import Flow\n",
        "\n",
        "def create_agent_flow():\n",
        "    \"\"\"\n",
        "    Create and connect the nodes to form a complete agent flow.\n",
        "\n",
        "    The flow works like this:\n",
        "    1. DecideAction node decides whether to search or answer\n",
        "    2. If search, go to SearchWeb node\n",
        "    3. If answer, go to AnswerQuestion node\n",
        "    4. After SearchWeb completes, go back to DecideAction\n",
        "\n",
        "    Returns:\n",
        "        Flow: A complete research agent flow\n",
        "    \"\"\"\n",
        "    # Create instances of each node\n",
        "    decide = DecideAction()\n",
        "    search = SearchWeb()\n",
        "    answer = AnswerQuestion()\n",
        "\n",
        "    # Connect the nodes\n",
        "    # If DecideAction returns \"search\", go to SearchWeb\n",
        "    decide - \"search\" >> search\n",
        "\n",
        "    # If DecideAction returns \"answer\", go to AnswerQuestion\n",
        "    decide - \"answer\" >> answer\n",
        "\n",
        "    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n",
        "    search - \"decide\" >> decide\n",
        "\n",
        "    # Create and return the flow, starting with the DecideAction node\n",
        "    return Flow(start=decide)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIwsNEDCxKBI",
        "outputId": "e6c02020-6fae-4377-8f0a-01d2580dd659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤔 Processing question: Who won the Nobel Prize in Physics 2024?\n",
            "🤔 Agent deciding what to do next...\n",
            "🔍 Agent decided to search for: 2024 Nobel Prize in Physics winner\n",
            "🌐 Searching the web for: 2024 Nobel Prize in Physics winner\n",
            "📚 Found information, analyzing results...\n",
            "🤔 Agent deciding what to do next...\n",
            "💡 Agent decided to answer the question\n",
            "✍️ Crafting final answer...\n",
            "✅ Answer generated successfully\n",
            "\n",
            "🎯 Final Answer:\n",
            "John J. Hopfield and Geoffrey Hinton won the 2024 Nobel Prize in Physics. They were awarded this prestigious recognition for their foundational discoveries and inventions that have significantly advanced the field of machine learning by enabling the use of artificial neural networks. These contributions have had a profound impact on the development and application of machine learning technologies.\n"
          ]
        }
      ],
      "source": [
        "# main.py\n",
        "import sys\n",
        "\n",
        "def main():\n",
        "    \"\"\"Simple function to process a question.\"\"\"\n",
        "    # Default question\n",
        "    default_question = \"Who won the Nobel Prize in Physics 2024?\"\n",
        "\n",
        "    # Get question from command line if provided with --\n",
        "    question = default_question\n",
        "    for arg in sys.argv[1:]:\n",
        "        if arg.startswith(\"--\"):\n",
        "            question = arg[2:]\n",
        "            break\n",
        "\n",
        "    # Create the agent flow\n",
        "    agent_flow = create_agent_flow()\n",
        "\n",
        "    # Process the question\n",
        "    shared = {\"question\": question}\n",
        "    print(f\"🤔 Processing question: {question}\")\n",
        "    agent_flow.run(shared)\n",
        "    print(\"\\n🎯 Final Answer:\")\n",
        "    print(shared.get(\"answer\", \"No answer found\"))\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
